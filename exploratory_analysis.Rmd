---
title: "PML Project - Exploratory Analysis"
author: "Tim Wise"
date: "March 2016"
output: 
  html_document:
    toc: yes
    toc_depth: 4
---

### Load libraries

```{r}
suppressMessages(library(AppliedPredictiveModeling))
suppressMessages(library(caret))
suppressMessages(library(rpart))
suppressMessages(library(tree))

#install.packages('assertthat')
suppressMessages(library(assertthat))
suppressMessages(library(dplyr))
suppressMessages(library(scales))

suppressMessages(library(doParallel))
```

### Start Cluster 

Turn on parallelism in hopes of speeding things up:
```{r}
r.cluster <- makeCluster(detectCores())
registerDoParallel(r.cluster)
```

### Download the dataset

Set the working directory manually.

Download the training data set, if necessary:
```{r}
url <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
csv <- './data/pml-training.csv'

if (!file.exists(csv)) {
  download.file(url, destfile=csv)
}
```


### Partition into training, test, and validation sets and write back to disk

Create a training, test, and validation sets (60%, 20%, 20%):
```{r}
rawDF <- read.csv(csv)
str(rawDF)

# set seed so splitting is reproducible
set.seed(3959648)

# 100% -> 60%, 40%
trainIndex <- createDataPartition(rawDF$classe, p = 0.60, list=FALSE)
trainSet          <- rawDF[ trainIndex,]
testValidationSet <- rawDF[-trainIndex,]

# 40% -> 20%, 40%
testIndex <- createDataPartition(testValidationSet$classe, p = 0.50, list=FALSE) 
testSet       <- testValidationSet[ testIndex,]
validationSet <- testValidationSet[-testIndex,]
```

Verify the sets seem different (that split correctly):
```{r}
# Should be about 60/20/20 split
dim(trainSet)
dim(testSet)
dim(validationSet)

# Should not be identical tables
table(trainSet$user,      trainSet$classe)
table(testSet$user,       testSet$classe)
table(validationSet$user, validationSet$classe)

# Should all be true
length(intersect(trainSet$Column1, testSet$Column1)) == 0
length(intersect(trainSet$Column1, validationSet$Column1)) == 0
length(intersect(testSet$Column1,  validationSet$Column1)) == 0
```

Write data sets to disk:
```{r}
trainCsv      <- './data/train.csv'
testCsv       <- './data/test.csv'
validationCsv <- './data/validation.csv'

if (!file.exists(trainCsv)) {  
  write.csv(file = trainCsv,      x = trainSet,     row.names=FALSE)
}  
if (!file.exists(testCsv)) {  
  write.csv(file = testCsv,       x = testSet,      row.names=FALSE)
}  
if (!file.exists(validationCsv)) {  
  write.csv(file = validationCsv, x = validationSet,row.names=FALSE)
}  
```

Read in and verify again:
```{r}
trainCsv      <- './data/train.csv'
testCsv       <- './data/test.csv'
validationCsv <- './data/validation.csv'

trainSet      <- read.csv(file = trainCsv)
testSet       <- read.csv(file = testCsv)
validationSet <- read.csv(file = validationCsv)

# Should be about 60/20/20 split
dim(trainSet)
dim(testSet)
dim(validationSet)

# Should not be identical tables
table(trainSet$user,      trainSet$classe)
table(testSet$user,       testSet$classe)
table(validationSet$user, validationSet$classe)

# Should all be true
assert_that(length(intersect(trainSet$Column1, testSet$Column1)) == 0)
assert_that(length(intersect(trainSet$Column1, validationSet$Column1)) == 0)
assert_that(length(intersect(testSet$Column1,  validationSet$Column1)) == 0)
```


### Clean the Training Set

#### Map Weird Values to NA

We see blanks, NA, and '#DIV/0!' values and this causes some numeric
fields to be read in as factors:
```{r}
trainSet <- read.csv('./data/train.csv')
str(trainSet)
```

Reread the file and map the wierd values to NA and suppress factor creation:
```{r}
trainSet <- read.csv('./data/train.csv', 
                     na.string = c('', 'NA', '#DIV/0!'),
                     stringsAsFactors = FALSE)
assert_that(length(grep('#DIV', summary(trainSet))) == 0)
str(trainSet)
```

#### Remove Time-Windowed Metrics

Reading the paper for the data set, we find there are three kinds of metrics:  
- Raw metrics reported by the sensors (accelerometer, gyroscope, and magnetometer)  
- Derived metrics for roll, pitch, and yaw  
- Summarizations (min, max, etc.) of the derived metrics  

The summarization metrics are not reported for every observation. 
They are only reported at the end of a sliding window.
Because of this, we are going to exclude the window summarizations from our 
analysis. 

We can do this by removing the end-of-window rows:
```{r}
table(trainSet$new_window)

trainSet <- filter(trainSet, new_window != 'yes')

table(trainSet$new_window)
assert_that(length(table(trainSet$new_window)) == 1)
```

Let's find all the columns that are entirely NA. 
These will be the window summary metrics:
```{r}
isNACol <- unlist(lapply(trainSet, function(x){all(is.na(x))}))
sort(names(trainSet)[isNACol]) 
```

Remove those columns:
```{r}
trainSet <- trainSet[!isNACol]
```

#### Remove Identifier Fields

Now let's remove fields that are indentifier fields. This includes:   
- row numbers  
- user ids  
- time period ids   
```{r}
ignoreCols <- c('Column1', 
                'user_name', 
                'raw_timestamp_part_1',
                'raw_timestamp_part_2',
                'cvtd_timestamp',
                'new_window',
                'num_window'
                )
trainSet <- trainSet[,!names(trainSet) %in% ignoreCols]
```

#### Factor the Outcome Variable

Finally, change `classe`, the outcome variable, to a factor:
```{r}
trainSet$classe <- as.factor(trainSet$classe)
```

#### The Final Training Set

Here's the data set we'll used to build our model:
```{r}
str(trainSet)
```

### Decide on the Acceptance Criteria

We are going to use *accurracy* as the acceptance measure.


### Potential Models

#### Classification Tree: rpart 

Call the rpart functions directly, rather than implicitly through 
caret/train:
```{r}
#ctrl <- <- rpart.control()

fit.rpart <- rpart(factor(classe) ~ .,  
                   data   = trainSet,  
                   method = "class")
#printcp(fit.rpart)   
plotcp(fit.rpart)
```

#### Classification Tree: rpart via train

Let's try a simple classification tree and see what kind of accuracy we get.
This is mostly to work out the process of modeling building and evaluation.
I expect we'll be doing random forests, SVDs, or some other more complex
algorithm for the final analysis.

Create model doing 10-fold cross-validation:
```{r}
trc <- trainControl(method = 'cv', number = 10)
#rpg <- rpart.grid <- expand.grid(.cp=0.0001)

rpart.model <- train(factor(classe) ~ ., 
                     data = trainSet, 
                     method    = "rpart",
                     trControl = trc)
#                     tuneGrid  = rpg)
rpart.model
rpart.model$finalModel
#printcp(rpart.model$finalModel)
```

See what the accuracy is on the test data set:
```{r}
rpart.predictions <- predict(rpart.model)
rpart.cm <- confusionMatrix(data = rpart.predictions, reference = trainSet$classe)
rpart.cm 
```
Wow, the overall accuracy on the training set is only `r percent(rpart.cm$overall["Accuracy"])`.
This won't cut it.

But let's see what the accuracy is on the test data set:
```{r}
```


#### Classification Tree: tree

Let's call tree() directly and see if we get anything different from rpart():
```{r}
set.seed(5321)
tree.model <- tree(factor(classe) ~ ., data=trainSet)
tree.model.summary <- summary(tree.model)
tree.model.summary
```
Creates a tree from `r length(tree.model.summary$used)` attributes 
having `r tree.model.summary$size` leaf nodes 
and has a missclassification error rate of 
`r round(tree.model.summary$misclass[1]/tree.model.summary$misclass[2], 4)`  


Let's look at the tree:
```{r}
plot(tree.model); text(tree.model)
```

Kinda ugly. 


Let's see how well it predicts on the test data 
[seems like this should be part of the tree result]:
```{r}
tree.predictions <- predict(tree.model, trainSet, type = 'class')
tree.cm <- confusionMatrix(data = tree.predictions, reference = trainSet$classe)
tree.cm 
```
Accuracy on training set is `r percent(tree.cm$overall["Accuracy"])`.
Much better than rpart(). Why?


Let's cross-validate (using 10 folds) using misclass and deviance as measures
and see if we should prune the tree:
```{r}
misclass.cv.tree.model <- cv.tree(tree.model, FUN = prune.tree, method = "misclass")
dev.cv.tree.model      <- cv.tree(tree.model)

par(mfrow=c(1,2))
plot(misclass.cv.tree.model)
plot(dev.cv.tree.model)
par(mfrow=c(1,1))
```
Both graphs continue descreasing, suggesting that the tree is not overfitting
and that most nodes are needed, say about 16.


Look at the summaries:
```{r}
misclass.cv.tree.model
dev.cv.tree.model
```

  
### Stop Cluster

Turn off parallelism:
```{r}
stopCluster(r.cluster)
```