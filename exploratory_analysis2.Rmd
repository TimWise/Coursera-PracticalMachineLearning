---
title: "PML Project - Exploratory Analysis - Cross-Validate by User"
author: "Tim Wise"
date: "April 2016"
output: 
  html_document:
    toc: yes
    toc_depth: 4
---

### Overview

In the real world, an algorithm like the one we're developing 
would be trained a one set of users and 
applied to a new set for a *different person*.

In this exercise, we'll partition the data the data by user. 
Then we'll do a cross-validation, creating a models from N-1 users and
testing in on the held out user. 
We'll do that for each user, giving an N-fold
cross-validation, where N is the number of users.

We'll average the error rates of the different models to get a 
cross-validated error rate and see if it's significantly different
from the random forest we previously constructed.

```{r}
set.seed(3959648)
```


### Load libraries

```{r}
suppressMessages(library(AppliedPredictiveModeling))
suppressMessages(library(caret))
suppressMessages(library(rpart))
suppressMessages(library(tree))
suppressMessages(library(randomForest))

#install.packages('assertthat')
suppressMessages(library(assertthat))
suppressMessages(library(dplyr))
suppressMessages(library(scales))

suppressMessages(library(doParallel))
```


### Start Cluster 

Turn on parallelism in hopes of speeding things up:
```{r}
r.cluster <- makeCluster(detectCores())
registerDoParallel(r.cluster)
```


### Download the datasets

Set the working directory manually, to the directory of this .rmd file.

Download the training, if necessary:
```{r}
url <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
csv <- './data/pml-training.csv'

if (!file.exists(csv)) {
  download.file(url, destfile=csv)
}
```


### Read in training set

Read in data using lessons learned from first analysis:  
- Map wierd values to NA  
- Suppress factor creation   
- Make outcome, classe, a factor

```{r}
allSet <- read.csv('./data/pml-training.csv', 
                     na.string = c('', 'NA', '#DIV/0!'),
                     stringsAsFactors = FALSE)

allSet$classe <- as.factor(allSet$classe)
```


### Create helper functions for cleaning training sets

Reading the paper for the data set, we find there are three kinds of metrics:  
- Raw metrics reported by the sensors (accelerometer, gyroscope, and magnetometer)  
- Derived metrics for roll, pitch, and yaw  
- Summarizations (min, max, etc.) of the derived metrics  

The summarization metrics are not reported for every observation. 
They are only reported at the end of a sliding window.
Because of this, we are going to exclude the window summarizations from our 
analysis. 

Clean the data:  
- remove 'new window' rows containing window summary metrics 
- remove columns that are 100% NA  
- remove key (non-metric) columns  


```{r}
cleanTraining <- function(inSet) {
  
  # leave user_name in as an identifier of the test set
  # remove user_name when creating model
  ignoreCols <- c('Column1', 
                'raw_timestamp_part_1',
                'raw_timestamp_part_2',
                'cvtd_timestamp',
                'new_window',
                'num_window'
                )
  inSet %>% 
    filter(new_window == 'no') %>%
    select(which(colMeans(is.na(.)) < 1.0))  %>%
    select(-one_of(ignoreCols)) -> outSet
    
  outSet 
} 
```


Helper functions to get error rate and accuracy from a random forest:
```{r}
rfErrorRate <- function(rf = NULL) {
  round(rf$err.rate[rf$ntree] * 100, 2)
}

rfAccuracy <- function(rf = NULL) {
  round(100 - rfErrorRate(rf), 2)
}

cmAccuracy <- function(cm = NULL) {
  round(cm$overall["Accuracy"] * 100, 2)
} 
```

### Do k-fold cross-validation of randomForest, once for each user


#### Round 1: Saving only forests and confusion results

[We were a little naive on list manipulation and just appended to the end.]

```{r}
userNames <- unique(allSet$user_name)

rfList <- list()
cmList <- list()
for (userName in userNames) {
  
  testSet  <- filter(allSet, user_name == userName)
  trainSet <- filter(allSet, user_name != userName)
  
  trainSet <- cleanTraining(trainSet)
  
  rf <- randomForest(factor(classe) ~ ., 
                     data = select(trainSet, -user_name), 
                     ntree = 150)
  
  predictions <- predict(rf, testSet)
  cm <- confusionMatrix(data = predictions, reference = testSet$classe)
  
  rfList <- c(rfList, list(rf))
  cmList <- c(cmList, list(cm))
}
```


The results of the randomForest training:
```{r}
sapply(rfList, print)
```

The confusion matrices for test predictions:                            :
```{r}
sapply(cmList, print)
```

The accuracy percentage[0-100] for each fold:
```{r}
df <-  data.frame(train = sapply(rfList, rfAccuracy), 
                  test  = sapply(cmList, cmAccuracy),
                  row.names = NULL)
df
```

Average accuracy [0-100] across all folds:
```{r}
colMeans(df)
```

Wow! **The model fails badly when trained on one set of users and applied to another.**
It seems the model needs to be trained on the set of users on which it will be tested

We can only hope that the final test set is drawn from the users we're training on, otherwise
we will not get 16/20 answers correct. 


#### Round 2: Hopefully more R-like, saving train/test sets, too

[This time we save and retrieve to/from named list elements where we
store the results for fold i.
The name of a fold is the user_name that is held for the test set.]

Initialize the lists where we'll save the results:
```{r}
userNames <- unique(allSet$user_name)

trainSets <- list()
testSets  <- list()
rfs <- list()
cms <- list()
```

Create helper function to execute a fold for a given user:  
- Partition into training and test sets by user name  
- Train random forest
- Test random forest

Save all the results (data sets, forests, confusion matrices). 

```{r}
cvDoFoldForUser <- function(userName) {
  
  testSets[[userName]]  <<- filter(allSet, user_name == userName)
  trainSets[[userName]] <<- filter(allSet, user_name != userName) %>% cleanTraining(.) 
  
  rfs[[userName]] <<- randomForest(factor(classe) ~ ., 
                                   data  = select(trainSets[[userName]], -user_name), 
                                   ntree = 150)
  
  predictions <- predict(rfs[[userName]], testSets[[userName]])
  cms[[userName]] <<- confusionMatrix(data = predictions, 
                                      reference = testSets[[userName]]$classe)
}
```

Execute the folds a fold, saving results to global vars:  
```{r}
devnull <- lapply(userNames, cvDoFoldForUser)
```

Check the size of the training sets:
```{r}
trainSets %>% 
  lapply(., function(df) c(nrows = nrow(df), ncols = ncol(df))) %>% 
  do.call(rbind,.)
```

Check the size of the test sets:
```{r}
testSets %>% 
  lapply(., function(df) c(nrows = nrow(df), ncols = ncol(df)))  %>%
  do.call(rbind,.)
```

Check users and exercise types in each of the training sets:
```{r}
trainSets %>% 
  lapply(., function(df) table(df$user_name, df$classe))
```

Check users and exercise types in the test sets:
```{r}
testSets %>% 
  lapply(., function(df) table(df$user_name, df$classe))
```

Summarize the accuracies for each fold:
```{r}
foldAccuracies <- data.frame(train = sapply(rfs, rfAccuracy), 
                             test  = sapply(cms, cmAccuracy))
foldAccuracies
```

Average accuracy [0-100] across all folds:
```{r}
overallAccuracies <- round(colMeans(foldAccuracies), 2)
overallAccuracies
```

Again, test accuracies are suprisingly bad.


### Takeaway: Use traditional cross-validation to develop the model


### Stop Cluster

Turn off parallelism:
```{r}
stopCluster(r.cluster)
```
